# 微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级
本文转载自： [微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级](https://www.ithome.com/0/867/391.htm)

感谢IT之家网友 [华南吴彦祖](https://m.ithome.com/html/app/open.html?url=ithome%3A%2F%2Fuserpage%3Fid%3D2029428) 的线索投递！

[IT之家](https://www.ithome.com/) 7 月 11 日消息，科技媒体 NeoWin 昨日（7 月 10 日）发布博文，报道称微软推出 Phi-4-mini-flash-reasoning 小语言模型，**重点提升端侧 AI 模型的数学和逻辑推理能力。**

Phi-4-mini-flash-reasoning 的主要优势在于，它能够在边缘设备、移动应用和嵌入式系统等资源不足的场景下，引入先进的推理功能。

![](https://pic.code-nav.cn/post_picture/1610518142000300034/XzTylyOrPZorVZAg.webp "微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级")

在架构方面，Phi-4-mini-flash-reasoning 创新引入了 SambaY 架构，而该架构的一大亮点，就是名为 Gated Memory Unit（GMU）的组件，它能够高效地在模型的内部之间共享信息，从而提高模型的效率。

这些改进让模型能够更快地生成答案和完成任务，即使面对非常长的输入也能应对自如，Phi 模型还能处理大量数据，理解非常长的文本或对话。

![](https://pic.code-nav.cn/post_picture/1610518142000300034/jVwW72UMPia8T1Jo.webp "微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级")

在性能方面，相比较其它 Phi 模型，Phi-4-mini-flash-reasoning 的吞吐量最高提升 10 倍，这意味着在给定的时间内，Phi-4-mini-flash-reasoning 可以完成更多的工作。

它可以在相同的时间内处理 10 倍多的请求或生成 10 倍多的文本，这对于实际应用来说是一个巨大的改进，此外，延迟也降低至其它 Phi 模型的 1/2~1/3。IT之家附上相关性能数据如下：

![](https://pic.code-nav.cn/post_picture/1610518142000300034/1GMi1QYKujJ1AAJ1.webp "微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级")

![](https://pic.code-nav.cn/post_picture/1610518142000300034/MrX7DJpTFgLwg2lp.webp "微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级")

![](https://pic.code-nav.cn/post_picture/1610518142000300034/dvvbGop501C8Zce1.webp "微软发布 Phi-4-mini-flash-reasoning 端侧 AI 模型：10 倍吞吐量，推理能力升级")

Phi-4-mini-flash-reasoning 新型模型已在 Azure AI Foundry、NVIDIA API Catalog 和 Hugging Face 上线。

广告声明：文内含有的对外跳转链接（包括不限于超链接、二维码、口令等形式），用于传递更多信息，节省甄选时间，结果仅供参考，IT之家所有文章均包含本声明。